<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <title>Project 4: Neural Radiance Field!</title>
  <meta name="description" content="">
  <meta name="keywords" content="">

  <!-- Favicons -->
  <link href="../assets/img/favicon.png" rel="icon">
  <link href="../assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Fonts -->
  <link href="https://fonts.googleapis.com" rel="preconnect">
  <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
  <link
    href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Ubuntu:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&family=Nunito:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap"
    rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="../assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="../assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="../assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="../assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="../assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Main CSS File -->
  <link href="../assets/css/main.css" rel="stylesheet">
</head>

<body class="portfolio-details-page">
  <main class="main">

    <!-- Page Title -->
    <div class="page-title dark-background">
      <div class="container d-lg-flex justify-content-between align-items-center">
        <h1 class="mb-2 mb-lg-0">
          <a href="/CS180/" class="navbar-title-link">Joel's CS180 Projects</a>
        </h1>

        <nav class="breadcrumbs">
          <ol>
            <li><a href="/CS180/">Joel's CS180 Projects</a></li>
            <li class="current">Project 4</li>
          </ol>
        </nav>
      </div>
    </div><!-- End Page Title -->

    <section id="project-0" class="project-section container py-5">

      <!-- Page Title -->
      <div class="section-title text-center">
        <h2>Project 4: Neural Radiance Field!</h2>
      </div>
      
      <!-- Part 0: Calibrating Your Camera -->
      <div class="project-part mb-5">
        <h3>Part 0: Calibrating Your Camera</h3>
        
        <p>We will first solve for the camera instrinsics of the smartphone that will be used to capture images for our NeRF. To do this we will use the following calibration pipeline:</p>
        
        <ol>
          <li>Loop through all your calibration images</li>
          <li>For each image, detect the ArUco tags using OpenCV's ArUco detector</li>
          <li>Extract the corner coordinates from the detected tags</li>
          <li>Collect all detected corners and their corresponding 3D world coordinates (you can consider the ArUco tag as the world origin and define the 4 corners' 3D points relative to that, e.g., if your tag is 0.02m × 0.02m, the corners could be [(0,0,0), (0.02,0,0), (0.02,0.02,0), (0,0.02,0)])</li>
          <li>Use cv2.calibrateCamera() to compute the camera intrinsics and distortion coefficients</li>
        </ol>
        
        <p>Here is a sample of four of the calibration images from a set of 40 captured from our smartphone:</p>
        
        <!-- Calibration Images Gallery -->
        <div class="gallery d-flex flex-wrap justify-content-center gap-4 my-4">
          <div class="gallery-card text-center">
            <a href="PXL_20251111_101424408.jpg" class="glightbox" title="arUco marker grid 1">
              <img src="PXL_20251111_101424408.jpg" alt="arUco marker grid 1" class="gallery-img">
            </a>
          </div>
          
          <div class="gallery-card text-center">
            <a href="PXL_20251111_101428628.jpg" class="glightbox" title="arUco marker grid 2">
              <img src="PXL_20251111_101428628.jpg" alt="arUco marker grid 2" class="gallery-img">
            </a>
          </div>
          
          <div class="gallery-card text-center">
            <a href="PXL_20251111_101432661.jpg" class="glightbox" title="arUco marker grid 3">
              <img src="PXL_20251111_101432661.jpg" alt="arUco marker grid 3" class="gallery-img">
            </a>
          </div>
          
          <div class="gallery-card text-center">
            <a href="PXL_20251111_101435057.jpg" class="glightbox" title="arUco marker grid 4">
              <img src="PXL_20251111_101435057.jpg" alt="arUco marker grid 4" class="gallery-img">
            </a>
          </div>
        </div>
        
        <p>Next, we'll capture images of an object next to a single ArUco tag. Here is a sample of 2 of these images from a captured set of 41:</p>
        
        <!-- Object Images Gallery -->
        <div class="gallery d-flex flex-wrap justify-content-center gap-4 my-4">
          <div class="gallery-card text-center">
            <a href="PXL_20251111_101830822.jpg" class="glightbox" title="fox 0">
              <img src="PXL_20251111_101830822.jpg" alt="fox 0" class="gallery-img">
            </a>
          </div>
          
          <div class="gallery-card text-center">
            <a href="PXL_20251111_101840878.jpg" class="glightbox" title="fox 1">
              <img src="PXL_20251111_101840878.jpg" alt="fox 1" class="gallery-img">
            </a>
          </div>

          <div class="gallery-card text-center">
            <a href="PXL_20251111_101757123.jpg" class="glightbox" title="fox 2">
              <img src="PXL_20251111_101757123.jpg" alt="fox 2" class="gallery-img">
            </a>
          </div>
        </div>
        
        <p>Finally, we can use our camera calibration to solve for the camera poses that explain the position of marker points in screenspace, for each of the 41 caputred images. Here are 2 screenshots of camera frustums visualized in Viser:</p>
        
        <!-- Camera Frustum Visualization Gallery -->
        <div class="gallery d-flex flex-wrap justify-content-center gap-4 my-4">
          <div class="gallery-card text-center">
            <a href="gallery/photo_dome3.png" class="glightbox" title="Tag-size set to 0.02mm for image(s) visibility">
              <img src="gallery/photo_dome3.png" alt="photodome0" class="gallery-img">
            </a>
            <p class="caption mt-2">Tag-size set to 0.02mm for image(s) visibility</p>
          </div>
          
          <div class="gallery-card text-center">
            <a href="gallery/photo_dome1.png" class="glightbox" title="Camera frustum visualization (tag-size=0.1mm; angle 1)">
              <img src="gallery/photo_dome1.png" alt="photodome1" class="gallery-img">
            </a>
            <p class="caption mt-2">Camera frustum visualization (tag-size=0.1mm; angle 1)</p>
          </div>
          
          <div class="gallery-card text-center">
            <a href="gallery/photo_dome2.png" class="glightbox" title="Camera frustum visualization (tag-size=0.1mm; angle 2)">
              <img src="gallery/photo_dome2.png" alt="photodome2" class="gallery-img">
            </a>
            <p class="caption mt-2">Camera frustum visualization (tag-size=0.1mm; angle 2)</p>
          </div>
        </div>
        
        <p>After undistorting the images (because NeRF assumes a perfect pinhole camera model without distortion), we'll build our image poses into a dataset of the following keys to later train our NeRF with:</p>
        
        <ul>
          <li><b>images_train:</b> numpy array of shape (N_train, H, W, 3) containing your undistorted training images (0-255 range, will be normalized when loaded)</li>
          <li><b>c2ws_train:</b> numpy array of shape (N_train, 4, 4) containing the camera-to-world transformation matrices for training images</li>
          <li><b>images_val:</b> numpy array of shape (N_val, H, W, 3) for validation images</li>
          <li><b>c2ws_val:</b> numpy array of shape (N_val, 4, 4) for validation camera poses</li>
          <li><b>c2ws_test:</b> numpy array of shape (N_test, 4, 4) for test camera poses (used for novel view rendering)</li>
          <li><b>focal:</b> float representing the focal length from your camera intrinsics (assuming fx = fy)</li>
        </ul>
      </div>

      <!-- Part 1: Fit a Neural Field to a 2D Image -->
      <div class="project-part mb-5">
        <h3>Part 1: Fit a Neural Field to a 2D Image</h3>
        
        <p>Neural Radiance Field (NeRF) represent a 3D space. But before jumping into 3D, let's first get familar with NeRF (and PyTorch) using a 2D example. In this section, we will create a neural field that can represent a 2D image and optimize that neural field to fit this image. More specifically, we create an Multilayer Perceptron (MLP) network with Sinusoidal Positional Encoding (PE) that takes in the 2-dim pixel coordinates, and output the 3-dim pixel colors. We will start with the following image of a (real life) fox:</p>
        
        <!-- Fox Image -->
        <div class="gallery d-flex flex-wrap justify-content-center gap-4 my-4">
          <div class="gallery-card text-center">
            <a href="gallery/fox.jpg" class="glightbox" title="real fox">
              <img src="gallery/fox.jpg" alt="real fox" class="gallery-img">
            </a>
          </div>
        </div>
        
        <p><b>Multilayer Perceptron (MLP):</b> An MLP is simply a stack of non linear activations (e.g., torch.nn.ReLU() or torch.nn.Sigmoid()) and fully connected layers (torch.nn.Linear()). We will be building an MLP with the structure in the figure below. Note that we have a Sigmoid layer at the end of the MLP to constrain the network output be in the range of (0, 1), as a valid pixel color.</p>
        
        <!-- MLP Figure -->
        <div class="gallery d-flex flex-wrap justify-content-center gap-4 my-4">
          <div class="gallery-card text-center">
            <a href="gallery/mlp_img.jpg" class="glightbox" title="MLP figure">
              <img src="gallery/mlp_img.jpg" alt="MLP figure" class="gallery-img">
            </a>
          </div>
        </div>
        
        <p><b>Sinusoidal Positional Encoding (PE):</b> PE is an operation that applies a serious of sinusoidal functions to the input cooridnates, to expand its dimensionality. Note we also additionally keep the original input in PE, so the complete formulation is:</p>
        
        <p>
          \[PE(x) = \{x, \sin(2^0\pi x), \cos(2^0\pi x), \sin(2^1\pi x), \cos(2^1\pi x), ..., \sin(2^{L-1}\pi x), \cos(2^{L-1}\pi x)\}\]
        </p>
        
        <p>To deal with GPU memory limits, we will use a dataloader to randomly sample N pixels at every iteration for training.</p>
        
        <p>Along with the MLP network and the dataloader, we define a loss function and optimizer as follows: we will use PSNR, derived from mean squared error loss (MSE) between the predicted color and groundtruth color, and the network with be trained using Adam with a learning rate of 1e-2. We will train for 1000 iterations with a batch size of 10k. PSNR is defined as follow:</p>
        
        <p>
          \[PSNR = 10 \cdot \log_{10}\left(\frac{1}{MSE}\right)\]
        </p>
        
        <p>The model of the above specification yilded the above images when each pixel location was ran through the model. Each predicted image is the result of one model with varying values of L in [4, 10, 12] and layer_width in [128, 256, 512]:</p>
        
        <!-- Hyperparameter Results Table -->
        <div class="table-responsive my-4">
          <table class="table table-bordered text-center">
            <thead>
              <tr>
                <th><b>L</b>, <b>layer_width</b></th>
                <th><b>128</b></th>
                <th><b>256</b></th>
                <th><b>512</b></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><b>L = 4</b></td>
                <td><a href="gallery/fox_image_preds/pred_L4_W128.png" class="glightbox"><img src="gallery/fox_image_preds/pred_L4_W128.png" alt="L4W128" class="img-fluid"></a></td>
                <td><a href="gallery/fox_image_preds/pred_L4_W256.png" class="glightbox"><img src="gallery/fox_image_preds/pred_L4_W256.png" alt="L4W256" class="img-fluid"></a></td>
                <td><a href="gallery/fox_image_preds/pred_L4_W512.png" class="glightbox"><img src="gallery/fox_image_preds/pred_L4_W512.png" alt="L4W512" class="img-fluid"></a></td>
              </tr>
              <tr>
                <td><b>L = 10</b></td>
                <td><a href="gallery/fox_image_preds/pred_L10_W128.png" class="glightbox"><img src="gallery/fox_image_preds/pred_L10_W128.png" alt="L10W128" class="img-fluid"></a></td>
                <td><a href="gallery/fox_image_preds/pred_L10_W256.png" class="glightbox"><img src="gallery/fox_image_preds/pred_L10_W256.png" alt="L10W256" class="img-fluid"></a></td>
                <td><a href="gallery/fox_image_preds/pred_L10_W512.png" class="glightbox"><img src="gallery/fox_image_preds/pred_L10_W512.png" alt="L10W512" class="img-fluid"></a></td>
              </tr>
              <tr>
                <td><b>L = 12</b></td>
                <td><a href="gallery/fox_image_preds/pred_L12_W128.png" class="glightbox"><img src="gallery/fox_image_preds/pred_L12_W128.png" alt="L12W128" class="img-fluid"></a></td>
                <td><a href="gallery/fox_image_preds/pred_L12_W256.png" class="glightbox"><img src="gallery/fox_image_preds/pred_L12_W256.png" alt="L12W256" class="img-fluid"></a></td>
                <td><a href="gallery/fox_image_preds/pred_L12_W512.png" class="glightbox"><img src="gallery/fox_image_preds/pred_L12_W512.png" alt="L12W512" class="img-fluid"></a></td>
              </tr>
            </tbody>
          </table>
        </div>
        
        <p>By visual inspection, we achieve a result nearest to the input when we set the hyperparameters L = 10 and layer_widh = 256. Let us proceed with these discovered values to reconstruct this image of our stitched fox from before, plotting predictions at training progression, as well as the PSNR curve:</p>
        
        <!-- Stitched Fox and PSNR -->
        <div class="gallery d-flex flex-wrap justify-content-center gap-4 my-4">
          <div class="gallery-card text-center">
            <a href="gallery/stitch_fox.png" class="glightbox" title="Stitched fox to 2D reconstruct">
              <img src="gallery/stitch_fox.png" alt="Stitched fox to 2D reconstruct" class="gallery-img">
            </a>
            <p class="caption mt-2">Stitched fox to 2D reconstruct</p>
          </div>
          
          <div class="gallery-card text-center">
            <a href="gallery/stitched_fox_PSNR_training.png" class="glightbox" title="PSNR Training Curve">
              <img src="gallery/stitched_fox_PSNR_training.png" alt="PSNR Training Curve" class="gallery-img">
            </a>
            <p class="caption mt-2">PSNR Training Curve</p>
          </div>
        </div>
        
        <!-- Training Progress Table -->
        <div class="table-responsive my-4">
          <table class="table table-bordered text-center">
            <thead>
              <tr>
                <th>Reconstruction at Iteration 0</th>
                <th>Reconstruction at Iteration 100</th>
                <th>Reconstruction at Iteration 200</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><a href="gallery/fox_train_progress/pred_iter0.png" class="glightbox"><img src="gallery/fox_train_progress/pred_iter0.png" alt="iter0" class="img-fluid"></a></td>
                <td><a href="gallery/fox_train_progress/pred_iter100.png" class="glightbox"><img src="gallery/fox_train_progress/pred_iter100.png" alt="iter100" class="img-fluid"></a></td>
                <td><a href="gallery/fox_train_progress/pred_iter200.png" class="glightbox"><img src="gallery/fox_train_progress/pred_iter200.png" alt="iter200" class="img-fluid"></a></td>
              </tr>
              <tr>
                <th>Reconstruction at Iteration 300</th>
                <th>Reconstruction at Iteration 400</th>
                <th>Reconstruction at Iteration 500</th>
              </tr>
              <tr>
                <td><a href="gallery/fox_train_progress/pred_iter300.png" class="glightbox"><img src="gallery/fox_train_progress/pred_iter300.png" alt="iter300" class="img-fluid"></a></td>
                <td><a href="gallery/fox_train_progress/pred_iter400.png" class="glightbox"><img src="gallery/fox_train_progress/pred_iter400.png" alt="iter400" class="img-fluid"></a></td>
                <td><a href="gallery/fox_train_progress/pred_iter500.png" class="glightbox"><img src="gallery/fox_train_progress/pred_iter500.png" alt="iter500" class="img-fluid"></a></td>
              </tr>
              <tr>
                <th>Reconstruction at Iteration 600</th>
                <th>Reconstruction at Iteration 700</th>
                <th>Reconstruction at Iteration 800</th>
              </tr>
              <tr>
                <td><a href="gallery/fox_train_progress/pred_iter600.png" class="glightbox"><img src="gallery/fox_train_progress/pred_iter600.png" alt="iter600" class="img-fluid"></a></td>
                <td><a href="gallery/fox_train_progress/pred_iter700.png" class="glightbox"><img src="gallery/fox_train_progress/pred_iter700.png" alt="iter700" class="img-fluid"></a></td>
                <td><a href="gallery/fox_train_progress/pred_iter800.png" class="glightbox"><img src="gallery/fox_train_progress/pred_iter800.png" alt="iter800" class="img-fluid"></a></td>
              </tr>
              <tr>
                <th>Reconstruction at Iteration 900</th>
                <th>Reconstruction at Iteration 1000</th>
                <th>Original</th>
              </tr>
              <tr>
                <td><a href="gallery/fox_train_progress/pred_iter900.png" class="glightbox"><img src="gallery/fox_train_progress/pred_iter900.png" alt="iter900" class="img-fluid"></a></td>
                <td><a href="gallery/fox_train_progress/pred_iter999.png" class="glightbox"><img src="gallery/fox_train_progress/pred_iter999.png" alt="iter1000" class="img-fluid"></a></td>
                <td><a href="gallery/stitch_fox.png" class="glightbox"><img src="gallery/stitch_fox.png" alt="original" class="img-fluid"></a></td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>
      
      <!-- Part 2: Fit a Neural Radiance Field from Multi-view Images -->
      <div class="project-part mb-5">
        <h3>Part 2: Fit a Neural Radiance Field from Multi-view Images</h3>
        
        <p>Now that we are familiar with using a neural field to represent a image, we can proceed to a more interesting task that using a neural radiance field to represent a 3D space, through inverse rendering from multi-view calibrated images. For this part we are going to use the Lego scene from the original <a href="https://www.matthewtancik.com/nerf">NeRF paper</a>, but with lower resolution images (200 x 200) and preprocessed cameras (downloaded from <a href="https://cal-cs180.github.io/fa25/hw/proj4/assets/lego_200x200.npz">here</a>). The following code can be used to parse the data. The figure on its right shows a plot of all the cameras, including training cameras in black, validation cameras in red, and test cameras in green.</p>
        
        <!-- NeRF Camera Plot -->
        <div class="gallery d-flex flex-wrap justify-content-center gap-4 my-4">
          <div class="gallery-card text-center">
            <a href="gallery/data_plot.png" class="glightbox" title="NeRF tractor cameras plot">
              <img src="gallery/data_plot.png" alt="NeRF tractor cameras plot" class="gallery-img">
            </a>
            <p class="caption mt-2">NeRF tractor cameras plot</p>
          </div>
        </div>
        
        <h4>Part 2.1: Create Rays from Cameras</h4>
        
        <p><b>Camera to World Coordinate Conversion.</b> The transformation between the world space \(\mathbf{X_w} = (x_w, y_w, z_w)\) and the camera space \(\mathbf{X_c} = (x_c, y_c, z_c)\) can be defined as a rotation matrix \(\mathbf{R}_{3\times3}\) and a translation vector \(t\):</p>
        
        <p>
          \[\begin{align} \begin{bmatrix} x_c \\ y_c \\ z_c \\ 1 \end{bmatrix} = \begin{bmatrix}
          \mathbf{R}_{3\times3} &
          \mathbf{t} \\ \mathbf{0}_{1\times3} & 1 \end{bmatrix} \begin{bmatrix} x_w \\ y_w \\ z_w \\ 1 \end{bmatrix}
          \end{align}\]
        </p>
        
        <p>in which \(\begin{bmatrix} \mathbf{R}_{3\times3} & \mathbf{t} \\ \mathbf{0}_{1\times3} & 1 \end{bmatrix}\) is called world-to-camera (w2c) transformation matrix, or extrinsic matrix. The inverse of it is called camera-to-world (c2w) transformation matrix.</p>
        
        <p><b>Pixel to Camera Coordinate Conversion.</b> Consider a pinhole camera with focal length \((f_x, f_y)\) and principal point \((o_x = \text{imageWidth} / 2, o_y = \text{imageHeight} / 2)\), it's intrinsic matrix \(\mathbf{K}\) is defined as:</p>
        
        <p>
          \[\begin{align}\mathbf{K} = \begin{bmatrix} f_x & 0 & o_x \\ 0 & f_y & o_y \\ 0 & 0 & 1 \end{bmatrix} \end{align}\]
        </p>
        
        <p>which can be used to project a 3D point \((x_c, y_c, z_c)\) in the camera coordinate system to a 2D location \((u, v)\) in pixel coordinate system:</p>
        
        <p>
          \[\begin{align} s \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = \mathbf{K} \begin{bmatrix} x_c \\ y_c \\ z_c
      \end{bmatrix} \end{align}\]
        </p>
        
        <p>in which \(s=z_c\) is the depth of this point along the optical axis.</p>
        
        <p><b>Pixel to Ray.</b> A ray can be defined by an origin vector \(\mathbf{r}_o \in \mathbb{R}^3\) and a direction vector \(\mathbf{r}_d \in \mathbb{R}^3\). In the case of a pinhole camera, we want to know the \(\{\mathbf{r}_o, \mathbf{r}_d\}\) for every pixel \((u, v)\). The origin \(\mathbf{r}_o\) of those rays is easy to get because it is just the location of the camera in world coordinates. For a camera-to-world (c2w) transformation matrix \(\begin{bmatrix} \mathbf{R}_{3\times3} & \mathbf{t} \\ \mathbf{0}_{1\times3} & 1 \end{bmatrix}\), the camera origin is simply the translation component:</p>
        
        <p>
          \[\begin{align} \mathbf{r}_o = \mathbf{t} \end{align}\]
        </p>
        
        <p>To calculate the ray direction for pixel \((u, v)\), we can simply choose a point along this ray with depth equal to 1 (\(s=1\)) and find its coordinate in world space \(\mathbf{X_w} = (x_w, y_w, z_w)\) using your previously implemented functions. Then the normalized ray direction can be computed by:</p>
        
        <p>
          \[\begin{align} \mathbf{r}_d = \frac{\mathbf{X_w} - \mathbf{r}_o}{||\mathbf{X_w} - \mathbf{r}_o||_2} \end{align}\]
        </p>
        
        <p>In Part 1, we have done random sampling on a single image to get the pixel color and pixel coordinates. Here we can build on top of that, and with the camera intrinsics & extrinsics, we can convert the pixel coordinates into ray origins and directions. Since we have multiple images now, we will sample N rays by first sampling M images, and then sample N // M rays from every image.</p>
        
        <p>After having rays, we also need to discritize each ray into samples that live in the 3D space. The simplist way is to uniformly create some samples along the ray (<code>t = np.linspace(near, far, n_samples)</code>). For the Lego scene that we have, we can set <code>near=2.0</code> and <code>far=6.0</code>. The actually 3D corrdinates can be accquired by \(\mathbf{x} = \mathbf{r}_o + \mathbf{r}_d * t\). However this would lead to a fixed set of 3D points, which could potentially lead to overfitting when we train the NeRF later on. On top of this, we want to introduce some small perturbation to the points only during training, so that every location along the ray would be touched upon during training. We will achieve this by sampling a random location between each sampling interval along a ray like so: <code>t = t + (np.random.rand(t.shape) * t_width)</code> where t is set to be the start of each interval. We will set n_samples to 32 for this initial Lego NeRF.</p>
        
        <h4>Part 2.3: Putting the Dataloading All Together</h4>
        
        <p>Similar to Part 1, we will write a dataloader that randomly sample pixels from multiview images. What is different with Part 1, is that now we will convert the pixel coordinates into rays in our dataloader, and return ray origin, ray direction and pixel colors from your dataloader. Here are some visualizations of this sampling code:</p>
        
        <p>Plot of the cameras, rays, and samples in 3D.</p>
        
        <!-- Viser Plot -->
        <div class="gallery d-flex flex-wrap justify-content-center gap-4 my-4">
          <div class="gallery-card text-center">
            <a href="gallery/viser_plot.png" class="glightbox" title="viser plot">
              <img src="gallery/viser_plot.png" alt="viser plot" class="gallery-img">
            </a>
          </div>
        </div>
        
        <p>Rays sampled only from one camera; rays stay within the camera frustum.</p>
        
        <!-- Single Camera Views -->
        <div class="table-responsive my-4">
          <table class="table table-bordered text-center">
            <thead>
              <tr>
                <th>Single camera</th>
                <th>Single camera (alt. perspective)</th>
                <th>Upper-left single camera</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><a href="gallery/single-camera-1.png" class="glightbox"><img src="gallery/single-camera-1.png" alt="single camera screenshot1" class="img-fluid"></a></td>
                <td><a href="gallery/single-camera-2.png" class="glightbox"><img src="gallery/single-camera-2.png" alt="single camera screenshot2" class="img-fluid"></a></td>
                <td><a href="gallery/upper-left-single-camera.png" class="glightbox"><img src="gallery/upper-left-single-camera.png" alt="upper-left single camera" class="img-fluid"></a></td>
              </tr>
            </tbody>
          </table>
        </div>
        
        <h4>Part 2.4: Neural Radiance Field</h4>
        
        <!-- MLP NeRF Figure -->
        <div class="gallery d-flex flex-wrap justify-content-center gap-4 my-4">
          <div class="gallery-card text-center">
            <a href="gallery/mlp_nerf.png" class="glightbox" title="mlp nerf">
              <img src="gallery/mlp_nerf.png" alt="mlp nerf" class="gallery-img">
            </a>
          </div>
        </div>
        
        <p>After having samples in 3D, we want to use the network to predict the density and color for those samples in 3D. Hence, we will create an MLP that is similar to Part 1, but with three changes:</p>
        
        <ul>
          <li>Input is now 3D world coordinates instead of 2D pixel coordinates, along side a 3D vector as the ray direction. And we are going to output not only the color, but also the density for the 3D points. In the radiance field, the color of each point depends on the view direction, so we are going to use the view direction as the condition when we predict colors. Note we use Sigmoid to constrain the output color within range (0, 1), and use ReLU to constrain the output density to be positive. The ray direction will also be encoded by positional encoding (PE) but can use less frequency (e.g., L=4) than the cooridnate PE (e.g., L=10).</li>
          <li>This MLP is deeper. We are now doing a more challenging task of optimizing a 3D representation instead of 2D. So we need a more powerful network.</li>
          <li>Injection of the input (after PE) to the middle of the MLP through concatenation. It's a general trick for deep neural network, that is helpful for it to not forgetting about the input.</li>
        </ul>
        
        <h4>Part 2.5: Volume Rendering</h4>
        
        <p>Next, given a set of RGB+Depth predictions for all values along a ray, how do we predict the RGB value corresponding to that ray when rendering from some view? For this, we will utilize discrete approximation of the volume rendering equaiton:</p>
        
        <p>
          \[\begin{align}
          \hat{C}(\mathbf{r})=\sum_{i=1}^N T_i\left(1-\exp \left(-\sigma_i \delta_i\right)\right) \mathbf{c}_i, \text { where
          } T_i=\exp
          \left(-\sum_{j=1}^{i-1} \sigma_j \delta_j\right) \end{align}\]
        </p>
        
        <p>where \(\mathbf{c}_i\) is the color obtained from our network at sample location \(i\), \(T_i\) is the probability of a ray not terminating before sample location \(i\), and \(1 - e^{-\sigma_i \delta_i}\) is the probability of terminating at sample location.</p>
        
        <p>To train this model, we use 1000 gradient steps, a batchsize of 10K rays per gradent step, an Adam optimizer with a learning rate of 5e-4. PSNR between the actual pixel values in the training set and the volume rendered pixel values computed from the model sample points is used for loss. The model with implemented with the PyTorch librairy.</p>
        
        <p>Similar to how before where we were learning the "field" that best explained all the RGB values, we are now learning the radiance field that best explains how each of the ray sample points are being added up to produce each pixel in our training set.</p>
        
        <h4>2.6: Rendering</h4>
        
        <!-- Training Progress Images -->
        <div class="gallery d-flex flex-wrap justify-content-center gap-4 my-4">
          <div class="gallery-card text-center">
            <a href="gallery/render_009_psnr_11.59.png" class="glightbox" title="PSNR 11.59">
              <img src="gallery/render_009_psnr_11.59.png" alt="psnr 1" class="gallery-img">
            </a>
          </div>
          <div class="gallery-card text-center">
            <a href="gallery/render_009_psnr_16.01.png" class="glightbox" title="PSNR 16.01">
              <img src="gallery/render_009_psnr_16.01.png" alt="psnr 2" class="gallery-img">
            </a>
          </div>
          <div class="gallery-card text-center">
            <a href="gallery/render_009_psnr_19.99.png" class="glightbox" title="PSNR 19.99">
              <img src="gallery/render_009_psnr_19.99.png" alt="psnr 3" class="gallery-img">
            </a>
          </div>
          <div class="gallery-card text-center">
            <a href="gallery/render_009_psnr_20.98.png" class="glightbox" title="PSNR 20.98">
              <img src="gallery/render_009_psnr_20.98.png" alt="psnr 4" class="gallery-img">
            </a>
          </div>
          <div class="gallery-card text-center">
            <a href="gallery/render_009_psnr_21.42.png" class="glightbox" title="PSNR 21.42">
              <img src="gallery/render_009_psnr_21.42.png" alt="psnr 5" class="gallery-img">
            </a>
          </div>
        </div>
        
        <!-- Loss and PSNR Plots -->
        <div class="table-responsive my-4">
          <table class="table table-bordered text-center">
            <thead>
              <tr>
                <th>Lego Loss Plot</th>
                <th>Lego Val PSNR Plot</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><a href="gallery/tractor_loss_plot.png" class="glightbox"><img src="gallery/tractor_loss_plot.png" alt="lego loss plot" class="img-fluid"></a></td>
                <td><a href="gallery/tractor_validation_psnr_plot.png" class="glightbox"><img src="gallery/tractor_validation_psnr_plot.png" alt="lego val psnr plot" class="img-fluid"></a></td>
              </tr>
            </tbody>
          </table>
        </div>
        
        <!-- Lego Videos -->
        <div class="gallery d-flex flex-wrap justify-content-center gap-4 my-4">
          <div class="gallery-card text-center">
            <img src="gallery/lego_200x200.gif" alt="lego_200x200 video1" class="gallery-img">
          </div>
          <div class="gallery-card text-center">
            <img src="gallery/lego_200x200(1).gif" alt="lego_200x200 video2" class="gallery-img">
          </div>
        </div>
        
        <p>This NeRF reached a validation PSNR of 22.31 in <code>gradient_steps=3000</code>. Other specifications include:</p>
        
        <pre><code>
          loss_fn = MSELoss
          near=0.0, far=6.0,
          batch_size=10000,
          n_samples_per_ray=32
        </code></pre>
        
        <h5>Train NeRF on [our] Lafufu data</h5>
        
        <p>Next, I trained a lafufu NeRF that reached PSNR 20.48 dB in <code>gradient_steps=2400</code>. Other specifications include:</p>
        
        <pre><code>
          loss_fn = MSELoss
          near=0.02, far=0.5,
          batch_size=10000,
          n_samples_per_ray=64
        </code></pre>
        
        <p>I choose near/far through trial and errror, loss function is as before, batch_size is aslo the same. Number of samples per ray has increased. This is because we have less training data; mor samples per ray can help keep the quality of the reconstruciton up in spite of this.</p>
        
        <!-- Lafufu Training Progress -->
        <div class="gallery d-flex flex-wrap justify-content-center gap-4 my-4">
          <div class="gallery-card text-center">
            <a href="gallery/2400lafufu(1).gif" class="glightbox" title="Novel Lafufu Views">
              <img src="gallery/2400lafufu(1).gif" alt="lafufu_2400 video2" class="gallery-img">
            </a>
          </div>
        </div>
        
        <div class="gallery d-flex flex-wrap justify-content-center gap-4 my-4">
          <div class="gallery-card text-center">
            <a href="gallery/render_002_psnr_12.70.png" class="glightbox" title="PSNR 12.70">
              <img src="gallery/render_002_psnr_12.70.png" alt="psnr 2" class="gallery-img">
            </a>
          </div>
          <div class="gallery-card text-center">
            <a href="gallery/render_002_psnr_18.65.png" class="glightbox" title="PSNR 18.65">
              <img src="gallery/render_002_psnr_18.65.png" alt="psnr 3" class="gallery-img">
            </a>
          </div>
          <div class="gallery-card text-center">
            <a href="gallery/render_002_psnr_20.30.png" class="glightbox" title="PSNR 20.30">
              <img src="gallery/render_002_psnr_20.30.png" alt="psnr 4" class="gallery-img">
            </a>
          </div>
          <div class="gallery-card text-center">
            <a href="gallery/render_002_psnr_21.18.png" class="glightbox" title="PSNR 21.18">
              <img src="gallery/render_002_psnr_21.18.png" alt="psnr 5" class="gallery-img">
            </a>
          </div>
        </div>
        
        <!-- Lafufu Loss and PSNR Plots -->
        <div class="table-responsive my-4">
          <table class="table table-bordered text-center">
            <thead>
              <tr>
                <th>Lafufu Loss Plot</th>
                <th>Lego Val PSNR Plot</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><a href="gallery/lafufu_mse_loss.png" class="glightbox"><img src="gallery/lafufu_mse_loss.png" alt="lafufu loss plot" class="img-fluid"></a></td>
                <td><a href="gallery/lafufu_val_psnr.png" class="glightbox"><img src="gallery/lafufu_val_psnr.png" alt="lafufu val psnr plot" class="img-fluid"></a></td>
              </tr>
            </tbody>
          </table>
        </div>

        <h5>Train NeRF on stitched fox data</h5>
        
        <p>Next, I trained a lafufu NeRF that reached PSNR 20.82 dB in <code>gradient_steps=3000</code>. Other specifications include:</p>
        
        <pre><code>
          loss_fn = MSELoss
          near=0.02, far=0.5,
          batch_size=10000,
          n_samples_per_ray=64
        </code></pre>
        
        <p>This training session saw success when rendering from training and validation poses as seen in the below examples:</p>

        <!-- 3x9 stitched-fox trainng renders grid (27 images total) -->
        <div class="table-responsive my-4">
         <div class="table-responsive my-4">
          <table class="table table-bordered text-center">
            <tbody>
              <!-- Row 1 -->
              <tr>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_000.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_000.png" alt="frame 000" class="img-fluid"></a></td>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_001.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_001.png" alt="frame 001" class="img-fluid"></a></td>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_004.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_004.png" alt="frame 004" class="img-fluid"></a></td>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_005.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_005.png" alt="frame 005" class="img-fluid"></a></td>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_006.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_006.png" alt="frame 006" class="img-fluid"></a></td>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_007.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_007.png" alt="frame 007" class="img-fluid"></a></td>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_010.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_010.png" alt="frame 010" class="img-fluid"></a></td>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_011.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_011.png" alt="frame 011" class="img-fluid"></a></td>
              </tr>
              
              <!-- Row 2 -->
              <tr>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_012.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_012.png" alt="frame 012" class="img-fluid"></a></td>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_015.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_015.png" alt="frame 015" class="img-fluid"></a></td>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_016.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_016.png" alt="frame 016" class="img-fluid"></a></td>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_017.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_017.png" alt="frame 017" class="img-fluid"></a></td>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_018.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_018.png" alt="frame 018" class="img-fluid"></a></td>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_019.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_019.png" alt="frame 019" class="img-fluid"></a></td>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_020.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_020.png" alt="frame 020" class="img-fluid"></a></td>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_021.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_021.png" alt="frame 021" class="img-fluid"></a></td>
              </tr>
              
              <!-- Row 3 -->
              <tr>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_022.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_022.png" alt="frame 022" class="img-fluid"></a></td>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_023.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_023.png" alt="frame 023" class="img-fluid"></a></td>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_024.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_024.png" alt="frame 024" class="img-fluid"></a></td>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_025.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_025.png" alt="frame 025" class="img-fluid"></a></td>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_026.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_026.png" alt="frame 026" class="img-fluid"></a></td>
                <td><a href="gallery/stitched_fox_nerf_renders_training/frame_027.png" class="glightbox"><img src="gallery/stitched_fox_nerf_renders_training/frame_027.png" alt="frame 027" class="img-fluid"></a></td>
                <td colspan="2"></td>
              </tr>
            </tbody>
          </table>

          <p>However, we struggled to find a pose to rotate the view around the origin about that would create consistently accurate novel views, suggesting that there was perhaps serious overfitting to training data while novel views struggled. See one such attempt below:</p>
          <p>The PSNR plots are exlcuded for this run as the author forgot to save this plot and is now out of Colab credits to do so (@Google please approve my student status :/ ).</p>
          <div class="gallery d-flex flex-wrap justify-content-center gap-4 my-4">
            <div class="gallery-card text-center">
              <a href="gallery/3000fox_data.gif" class="glightbox" title="Novel Lafufu Views">
                <img src="gallery/3000fox_data.gif" alt="lafufu_2400 video2" class="gallery-img">
              </a>
            </div>
          </div>

          <p>NeRF or nothing as the kids say.</p>
        </div>
      </div>
    </section>

  </main>

  <footer id="footer" class="footer position-relative">

    <div class="container">
      <div class="copyright text-center ">
        <p>© <span>Copyright 2026.</span> <span>Joel's CS180 Projects.</span></p>
      </div>
    </div>

  </footer>

  <!-- Scroll Top -->
  <a href="#" id="scroll-top" class="scroll-top d-flex align-items-center justify-content-center"><i
      class="bi bi-arrow-up-short"></i></a>

  <!-- Preloader -->
  <div id="preloader"></div>

  <!-- Vendor JS Files -->
  <script src="../assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="../assets/vendor/php-email-form/validate.js"></script>
  <script src="../assets/vendor/aos/aos.js"></script>
  <script src="../assets/vendor/typed.js/typed.umd.js"></script>
  <script src="../assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="../assets/vendor/waypoints/noframework.waypoints.js"></script>
  <script src="../assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="../assets/vendor/imagesloaded/imagesloaded.pkgd.min.js"></script>
  <script src="../assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="../assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <!-- Main JS File -->
  <script src="../assets/js/main.js"></script>

</body>

</html>